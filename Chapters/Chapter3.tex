% Chapter Template

\chapter{Фрактальний аналіз} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

Наступним етапом цієї роботи є обчислення фрактальної розмірності для попередньо оброблених зображень. Таким чином, для кожного \textit{пацієнта} буде побудовано набір фрактальних характеристик.

\section{Фрактальна характеристика}
\subsection{Теоретичні відомості}

Для визначення поняття фракталу треба спочатку навести визначення розмірності Хаусдорфа. Нехай $\Omega$ -- обмежена множина у метричному просторі $X$.

\begin{defn}
	Нехай $\epsilon > 0$. Не більш ніж зліченне сімейство $\left\{ \omega_i \right\}_{i \in I}$ підмножин простору $X$ будемо називати \textbf{$\epsilon \textit{-покриттям}$} множини $\Omega$, якщо виконуються наступні умови:
	\begin{enumerate}[label=(\arabic*),ref=(\arabic*)]
		\item $\Omega \subset \bigcup\limits_{i \in I}{\omega_i}$
		\item $\forall i \in I: |\omega_i| < \epsilon$
	\end{enumerate}
	де $|\omega_i|$ -- діаметр множини $\omega_i$.
\end{defn}

Нехай $\alpha > 0$. Нехай $\Theta = \left\{ \omega_i \right\}_{i \in I}$ -- покриття множини $\Omega$. Визначимо наступну функцію, яка в деякому плані визначає розмір цього покриття: 
$$F_\alpha(\Theta):=\sum\limits_{i\in I} |\omega_i|^\alpha$$
Позначимо через $M^{\varepsilon}_{\alpha}(\Omega)$ \enquote{мінімальний розмір} $\epsilon$-покриття множини $\Omega$:
$$M^{\varepsilon}_{\alpha}(\Omega) := \inf(F_\alpha(\Theta))$$
де інфімум береться по всіх $\epsilon$-покриттях множини $\Omega$. Очевидно, що функція $M^{\varepsilon}_{\alpha}(\Omega)$ не спадає при зменшенні $\epsilon$, оскільки при зменшенні $\epsilon$ ми також звужуємо множину можливих $\epsilon$-покриттів. Отже, у неї є скінченна або нескінченна границя при $\varepsilon\rightarrow 0+$:
$$M_{\alpha}(\Omega)=\lim\limits_{\varepsilon\rightarrow 0+}M^{\varepsilon}_{\alpha}(\Omega)$$

\begin{defn}
	Величину $M_{\alpha}(\Omega)$ називають \textbf{$\alpha$-мірою Хаусдорфа} множини $\Omega$.
\end{defn}

З властивостей $\alpha$-міри Хаусдорфа слід визначити те, що $M_{\alpha}(\Omega)$ спадає по $\alpha$. Більш того для будь-якої множини $\Omega$ існує критичне значення $\alpha_0$, таке, що:
\begin{enumerate}
	\item $M_{\alpha}(\Omega)=0$ для всіх $\alpha>\alpha_0$
	\item $M_{\alpha}(\Omega)=+\infty$ для всіх $\alpha<\alpha_0$
\end{enumerate}

Значення $M_{\alpha_0}(\Omega)$ може бути нульовим, скінченно додатнім або нескінченним.

\begin{defn}
	Число $\alpha_0$ називають \textbf{розмірністю Хаусдорфа} множини $\Omega$
\end{defn}

Наведемо означення фракталу (або фрактальних множин), запропонований Мандельбротом \citep{book:mandelbrot}: 

\begin{defn}
	\emph{(Мандельброт)}
	Множина називається \textbf{фракталом}, якщо її розмірність Хаусдорфа строго перевищує його топологічну розмірність.
\end{defn} 


\subsection{Алгоритм box-counting}

Як правило, фрактальні множини мають складну геометричну структуру, а також мають властивість самоподібності. Характеристика, що описує цю властивість, є фрактальною розмірністю \citep{book:voss}.

При вимірюванні фрактальної розмірності різних природних і штучних
об'єктів виникає ряд проблем, пов'язаних з тим, що існує кілька визначень фрактальної розмірності. Базовим поняттям є розмірність Хаусдорфа, але її обчислення часто виявляється досить непростою задачею. Тому на практиці частіше використовуються інші розмірності, що відносяться до так званого класу box-computіng (або box-countіng) \citep{bib:boxcount}. Цей метод полягає у тому, що для довільного додатного $\delta$ обчислюється деяка функція $M_{\delta}(\Omega)$. Якщо $M_{\delta}(\Omega) \propto \delta^{-D}$, то множина $\Omega$ має фрактальну розмірність $D$. З пропорції випливає, що

$$\dim_{B}\Omega = D = \lim_{\delta \rightarrow 0} \frac{\log(M_{\delta}(\Omega))}{-\log(\delta)},$$

де значення $M_{\delta}(\Omega)$ дорівнює кількості $S$-вимірних кубів зі сторонами $\delta$, необхідних для покриття множини $\Omega$. 

\begin{defn}
	Число $\dim_{B}\Omega$ називають \emph{box-counting розмірністю} множини $\Omega$.
\end{defn}

Зауважимо, що $\dim_{B}\Omega$ не завжди може існувати. Зв'язок між box-counting розмірністю $\dim_{B}\Omega$ та розмірністю Хаусдорфа $\dim_{H}\Omega$ виражається наступною теоремою:

\begin{thm}
	$\dim_{B}\Omega \leq \dim_{H}\Omega$
\end{thm}


\subsection{Лінійна регрєсія}

Як зазначено у попередньому пункті, використання алгоритму обчислення фрактальної розмірності box-countіng вимагає перевірки пропорції $M_{\delta}(\Omega) \propto \delta^{-D}$, а отже, і обчислення регресійної прямої. Кутовий коефіцієнт цієї прямої буде дорівнювати $\dim_{B}\Omega$. Пряму регресії будемо шукати методом найкращого середньоквадратичного наближення -- треба знайти такий елемент $\Phi_0$, який мінімізує значення  $\sum_{i=0}^{n}{\left( \Phi_0(x_i) - y_i \right)^2}$.

Нехай $M_n$ – лінійна оболонка базису $\left\{ \phi_0, \phi_1, \dots \phi_n\right\}$, за яким ми шукаємо елемент найкращого середньоквадратичного наближення $\Phi_0$. Його існування для випадку гільбертового простору $H$ випливає за теоремою. Нехай ми шукаємо функцію найкращого середньоквадратичного наближення для функції $f$, $\left( \cdot, \cdot \right)$ – внутрішній добуток в даному гільбертовому просторі (у нашому випадку в якості такого простору логічно взяти простір неперервних функцій на області визначення функції $f$). Тоді за теоремою: $$\forall i \in M_n \left( f-\Phi_0, \Phi\right)=0.$$ Тоді $$\forall i \in \left\{ 0, 1, \dots n\right\} \left( f-\Phi_0, \phi_i\right)=0.$$ Нехай $$\Phi_0 = \sum_{i=0}^n {c_i \phi_i},$$ тоді $$\forall i \in \left\{ 0, 1, \dots n\right\} \left( f-\sum_{j=0}^n c_j \phi_j, \phi_i \right) = 0,$$ $$\sum_{j=0}^n {c_j \left(\phi_j, \phi_i \right)} = \left(f, \phi_i \right), i \in \left\{ 0, 1, \dots n \right\}.$$

Таким чином ми одержали систему лінійних алгебраїчних рівнянь з матрицею, що є матрицею Грамма $G = {\|\left(\phi_i, \phi_j \right)\|}_{i, j \in \left\{ 0, 1, \dots n\right\}}.$ Отже, якщо $\det\left(G\right) = 0$, то можна знайти ров'язок даної системи (в нашому випадку використовується метод Гауса), а отже і шукану функцію $\Phi_0$.

Отже, регрєсійну пряму можна знайти, побудувавши елемент найкращого середньоквадратичного наближення $\Phi_0$, обравши базис $\left\{\phi_0, \phi_1 \right\} = \left\{1, x \right\}$.

\section{Класифікація пацієнтів}

Слід помітити, що кожному пацієнту ставиться у відповідність набір (вибірка) фрактальних характеристик, розмір якої не є сталим. Отже, для порівняння набору характеристик різних пацієнтів, треба використовувати міру близькості між вибірками. 

\par
Оскільки міра близькості не є метрикою, вибір методів класифікації є дуже обмеженим. В цій роботі було використано найпростіший спосіб -- метод $k$ найближчих сусідів.

\subsection{Міра близькості та $p \textup{-статистика}$}

Через $H$ позначимо гіпотезу про рівность неперервних функцій розподілу $F_{G}(u)$ і $F_{G'}(u)$ генеральних сукупностей $G$ і $G'$ відповідно. Нехай $x = \left( x_1, x_2, \dots x_n \right) \in G$ і $x' = \left( x'_1, x'_2, \dots x'_m \right) \in G'$, $x_{(1)} \leq \dots \leq x_{(n)}$, $x_{(1)} \leq \dots \leq x_{(m)}$ -- порядкові статистики. Припустимо що $F_{G}(u) = F_{G'}(u)$. Позначимо через $A_{ij}^{(k)}, k = 1, 2, \dots m$ випадкова подія, яка полягає у тому, що $x'$ потрапляє в інтервал $\left( x_{(i)}, x_{(j)} \right)$, тобто $A_{ij}^{(k)} = \left\{ x'_k \in \left( x_{(i)}, x_{(j)} \right) \right\}$. Як відомо, ймовірність цієї події обчислюється за формулою:
$$P(A_{ij}^{(k)}) = P\left\{ x'_k \in \left( x_{(i)}, x_{(j)} \right) \right\} = p_{ij}^{(n)} = \frac{j-i}{n+1} = \frac{q}{n+1}, \quad q = j-i.$$

Покладемо 
$$ p_{ij}^{(1)} = \frac{ h_{ij}^{(n)}m + \frac{1}{2}g^2 - g\sqrt{h_{ij}^{(n)}(1-h)m + \frac{1}{4}g^2} }{ m+g^2 },$$
$$ p_{ij}^{(2)} = \frac{ h_{ij}^{(n)}m + \frac{1}{2}g^2 + g\sqrt{h_{ij}^{(n)}(1-h)m + \frac{1}{4}g^2} }{ m+g^2 },$$
де $h_{ij}^{(n)}$ -- частота події $A_{ij}^{(k)}$ в $m$ експериментів, величина $g = 3$.

Позначимо через $N$ кількість усіх довірчіх інтервалів $I_{ij}^{(n,m)} = \left( p_{ij}^{(1)}, p_{ij}^{(2)} \right)$ (тобто $N = {n(n-1)}/{2}$) і $L$ -- кількість інтервалів $I_{ij}^{(n,m)}$, які містять ймовірності $p_{ij}^{(n)}$. Покладемо $h^{(n,m)} = \rho(F^{*}, F^{*'}) = \rho(x, x') = {L}/{N}$. Оскільки $h^{(n,m)}$ -- частота випадкової події $B = \left\{ p_{ij}^{(n)} \in I_{ij}^{(n,m)} \right\}$, ймовірність якої дорівнює $p(B) = 1-\beta$, тоді, вважаючи що $h^{n,m}=h^{(n)}$, $m=N$ та $g=3$, отримаємо довірчий інтервал $I^{(n,m} = \left( p^{(1)}, p^{(2)}\right)$ для ймовірності $p(B)$. Статистика $h^{(n)}$ називається $p$-статистикою. Вона також є мірою близькості $\rho(x, x')$ між вибірками $x$ та $x'$ \parencite{bib:pstatistics}.


\subsection{Метод k найближчих сусiдiв. Кросвалідація.}

Використовуючи Метод k найближчих сусiдiв для класифікації пацієнтів, необхідно зробити припущення коректності постулату про компактність:

\begin{conj}
	переважна бiльшiсть об’єктiв, що належать до одного класу, є ближчими один
	до одного, нiж до об’єктiв iншого класу, i лежать в областi з вiдносно простою межею.
\end{conj}

Суть цього методу для класифікації елементу $x$ полягає у тому, що з навчальної вибірки обираємо $k$ найближчих (у розумінні міри близькості) до $x$ елементів (характеристик пацієнтів) та відносимо $x$ до домінантного класу з цих $k$ елементів.

Для пошуку оптимального числа $k$, а також розміру навчальної вибірки $R$ використовують метод \textbf{кросвалідації}. Суть цього методу заключається у тому, що для кожного значення $k$ та $R$ випадковим чином підбирається набір даних для навчання.


\section{Результати та спостереження}

Вхідний набір даних для дослідження кладається з знімків 6751 інтерфазних ядер букального епітелія, для кожного було зроблено 3 знімки мікроскопу: без фільтру, через жовтий фільтр та через пурпурного фільтру (отже всьго 20253 фотографії), взятого з 130 пацієнтів, з них 68 хворих раком, 29 здорових та 33 хворих іншою хворобою. У таблиць (Мал. \ref{fig:table1} - \ref{fig:table4}) приведено результати кросвалідації.

Отже, при $k = 12$ та $R = 0.8$ досягається найкраща чутливість, а при $k = 6$ та $R = 0.9$ досягається найкраща специфічність.